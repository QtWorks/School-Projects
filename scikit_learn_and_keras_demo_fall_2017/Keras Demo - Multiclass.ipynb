{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setting up the Data\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\n",
    "Note: use Shift+Enter to run the codeblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib figure size and make inline\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into a training & testing set\n",
    "\n",
    "`x_train` & `x_test` are the examples, and `y_train` & `y_test` are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# normalize the pixel values to be in the range (0, 1)\n",
    "x_train /= 255 \n",
    "x_test  /= 255\n",
    "\n",
    "#print(x_train.shape)\n",
    "x_validation = x_train[58000:]\n",
    "y_validation = y_train[58000:]\n",
    "x_train = x_train[:58000]\n",
    "y_train = y_train[:58000]\n",
    "#y_validation = binary_data.iloc[10000:11000]['label'].as_matrix()\n",
    "\n",
    "\n",
    "print(\"Number of training samples in full dataset: \" + str(x_train.shape[0]))\n",
    "print(\"Number of testing samples in full dataset: \" + str(x_test.shape[0]))\n",
    "print(\"Number of validation samples in full dataset: \" + str(x_validation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what our data looks like\n",
    "\n",
    "Let's plot one of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[0]\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "image = np.reshape(image, [img_rows, img_cols])\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# The shape of each image is a vector with 784 binary values (\"pixels\")\n",
    "image_shape = x_train[0].shape\n",
    "print(\"The shape of each image is\" + str(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification!\n",
    "\n",
    "###  Format the labels\n",
    "\n",
    "The labels are currently formatted as numeric values, ie the image above has the label `5`.\n",
    "\n",
    "Since we're training 10 binary classifiers, we need to convert the labels into binary values.\n",
    "\n",
    "ie, the label `1` becomes the array `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`. The entry in the \"1th\" position is true, and all others are false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying 10 digits\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "print(\"shape of our labels before\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]) + '\\n')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "y_validation = keras.utils.to_categorical(y_validation, num_classes)\n",
    "\n",
    "print(\"shape of our labels after\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is good to go, let's train our 10 classifiers!\n",
    "\n",
    "\n",
    "### Let's make our model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg_model = Sequential()\n",
    "logistic_reg_model.add(Dense(num_classes, activation='softmax', input_shape=image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model & store accuracies!\n",
    "\n",
    "Note: turning on verbosity can slow things down *a lot* so if you're running some large model, turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = logistic_reg_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_validation, y_validation))\n",
    "\n",
    "\n",
    "score = logistic_reg_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Learning MNIST with a Multilayer Network!\n",
    "\n",
    "We'll set up a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(512, activation='relu', input_shape=image_shape))\n",
    "mlp_model.add(Dense(256, activation='relu'))\n",
    "mlp_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the MLP\n",
    "\n",
    "We'll compile with the same parameters as before so that it's a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = mlp_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_validation, y_validation))\n",
    "\n",
    "score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# How deep can you go: can adding more layers help us?\n",
    "\n",
    "Creating a deep model gives us some performance gains, but it's not a panacea.  Let's see how model performance behaves as we add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size =128, verbose=False):\n",
    "    # type: (np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int, bool) -> List[float]\n",
    "    \"\"\"This method creates deep feedforward networks of varyig depth from 1 layer to max_layers.  It\n",
    "    returns a list of accuracies on the test set\n",
    "    \n",
    "    Args:\n",
    "        x_train: An np.ndarray of data where each row is a new data point\n",
    "        y_train: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        x_test: An np.ndarray of data where each row is a new data point\n",
    "        y_test: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        max_layers: An int the maximum depth to try\n",
    "        layer_size: The number of neurons in each layer\n",
    "    \n",
    "    Returns:\n",
    "        A List of floats as the accuracies of the model on the test set\n",
    "    \"\"\"\n",
    "    acc_scores = []\n",
    "    \n",
    "    # Build up and test 1 - max layers\n",
    "    for i in range(0, max_layers):\n",
    "        mlp_model = Sequential()\n",
    "        mlp_model.add(Dense(layer_size, activation='relu', input_shape=x_train[0].shape))\n",
    "        \n",
    "        # Add i layers\n",
    "        for j in range(1, i):\n",
    "            mlp_model.add(Dense(layer_size, activation='relu'))\n",
    "\n",
    "        mlp_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "        mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "        history = mlp_model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_validation, y_validation))\n",
    "        score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        # Print out scores if verbosity is non 0\n",
    "        if verbose:\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "        acc_scores.append(score)\n",
    "    \n",
    "    return acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_layers = 30\n",
    "layer_size = 128\n",
    "scores = depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size=layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, max_layers + 1), list(map(lambda x: x[1], scores)), 'o-')\n",
    "plt.title('Behaviour of model accuracy with depth')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, max_layers + 1), list(map(lambda x: x[0], scores)), 'o-')\n",
    "plt.title('Behaviour of model loss with depth')\n",
    "plt.ylabel('Testing loss')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "plt.tight_layout()\n",
    "plt.savefig('depth_experiment_{}-layers_{}-layer_size.png'.format(max_layers, layer_size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:\n",
    "\n",
    "<img src=\"depth_experiment_30-layers_128-layer_size.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# If adding more layers isn't helping, can we get smarter about the network?\n",
    "\n",
    "There are many different types of layers that we can use when building a network.  Right now we have just seen dense layers which are simply perceptron-like nodes stacked together.  Here we will use a set of layers that are well adapted to images called convoluational, and pooling layers.  If you are interested in understanding how these work check out this great paper https://arxiv.org/pdf/1603.07285.pdf.  For now, you can think of the convlutional layers as learning filter like features of increasing complexity.  We will need to redefine our data in order to work with these layers.  Convolutions work on a matrix of values as opposed to a flattened array, because the filters take into consideration local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "image_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_validation = x_train[58000:]\n",
    "y_validation = y_train[58000:]\n",
    "x_train = x_train[:58000]\n",
    "y_train = y_train[:58000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:  ', x_validation.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_validation = keras.utils.to_categorical(y_validation, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=image_shape))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNN takes a long time, cutting to the chase:\n",
    "Use GPU version of Keras for faster training.\n",
    "\n",
    "Training:\n",
    "<img src=\"cnn_train.png\" width=\"800\">\n",
    "\n",
    "Testing:\n",
    "<img src=\"cnn_test.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# State of the Art-ish Accuracy\n",
    "\n",
    "The state of the art on MNIST is 99.79% accuracy.  With a little extra work we can push our conv net pretty close to this at ~99.16% accuracy.  We can do this by adding regularization in the form of drop out and a better optimizer in the form of ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best CNN\n",
    "best_cnn_model = Sequential()\n",
    "best_cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                   activation='relu',\n",
    "                   input_shape=image_shape))\n",
    "best_cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "best_cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Flatten())\n",
    "best_cnn_model.add(Dense(128, activation='relu'))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_cnn_model.fit(x_train, y_train,\n",
    "                             batch_size=128,\n",
    "                             epochs=num_epochs,\n",
    "                             verbose=1,\n",
    "                             validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = best_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also very slow, here are the results:\n",
    "\n",
    "Training:\n",
    "<img src=\"best_train.png\" width=\"800\">\n",
    "\n",
    "Testing:\n",
    "<img src=\"best_test.png\" width=\"800\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Cool stuff with neural nets\n",
    "\n",
    "- Michael Nielsen's book is awesome: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "- Lots of keras tutorials: https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "- \"Deep art\": https://deepart.io/\n",
    "\n",
    "- Adversarial Networks: https://blog.openai.com/adversarial-example-research/\n",
    "\n",
    "- NN visualization: http://playground.tensorflow.org/\n",
    "\n",
    "- Generated cat images from outlines: https://affinelayer.com/pixsrv/\n",
    "\n",
    "- AlphaGo: https://deepmind.com/research/alphago/\n",
    "\n",
    "- Music Generation: https://magenta.tensorflow.org/welcome-to-magenta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
